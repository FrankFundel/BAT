{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770c486e",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817d5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../datasets/')\n",
    "from prepare_sequences import prepare, classes13\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c935447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:10<00:00,  1.21it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:04<00:00,  2.78it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:02<00:00,  4.46it/s]\n"
     ]
    }
   ],
   "source": [
    "patch_len = 44                               # = 250ms ~ 25ms\n",
    "patch_skip = 22                              # = 150ms ~ 15ms\n",
    "patch_skip = 22                              # = 150ms ~ 15ms\n",
    "seq_len = 60                                 # = 500ms with ~ 5 calls\n",
    "seq_skip = 15\n",
    "\n",
    "X_train, Y_train, X_test, Y_test, X_val, Y_val = prepare(\"../datasets/prepared.h5\", classes13, patch_len, patch_skip, seq_len, seq_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bfd423c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 7621\n",
      "(4492, 60, 44, 257) (4492,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total sequences:\", len(X_train) + len(X_test) + len(X_val))\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43a6df",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786cc28",
   "metadata": {},
   "source": [
    "Sequence -> pos. encoding -> Transformer (CLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94314cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299c78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mixedprecision = False\n",
    "use_sampler = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "180d0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "445b6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier based on a pytorch TransformerEncoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_len,\n",
    "        d_model,\n",
    "        num_classes,\n",
    "        nhead=8,\n",
    "        dim_feedforward=2048,\n",
    "        num_layers=6,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        classifier_dropout=0.1,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=-2, end_dim=-1)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=d_model,\n",
    "            dropout=dropout,\n",
    "            max_len=max_len,\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b6edd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, epoch, criterion, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, (inputs, labels) in enumerate(tqdm.tqdm(dataloader)):\n",
    "        # Transfer Data to GPU if available\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "         \n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(enabled=use_mixedprecision):\n",
    "            # Forward Pass\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate Loss\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += (predictions == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / num_samples\n",
    "    epoch_acc = running_corrects / num_samples\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2faac03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, epoch, criterion, optimizer, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for batch, (inputs, labels) in enumerate(tqdm.tqdm(dataloader)):\n",
    "            # Transfer Data to GPU if available\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update Weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate Loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += (predictions == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / num_samples\n",
    "        epoch_acc = running_corrects / num_samples\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab67bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsampler import ImbalancedDatasetSampler\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "\n",
    "train_data = TensorDataset(torch.Tensor(X_train), torch.from_numpy(Y_train))\n",
    "test_data = TensorDataset(torch.Tensor(X_test), torch.from_numpy(Y_test))\n",
    "val_data = TensorDataset(torch.Tensor(X_val), torch.from_numpy(Y_val))\n",
    "\n",
    "if use_sampler:\n",
    "    train_loader = DataLoader(train_data, sampler=ImbalancedDatasetSampler(train_data), batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data, sampler=ImbalancedDatasetSampler(test_data), batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_data, sampler=ImbalancedDatasetSampler(val_data), batch_size=batch_size)\n",
    "else:\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfcc650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = Net(\n",
    "    max_len=seq_len,\n",
    "    d_model=44*257,\n",
    "    num_classes=13,\n",
    "    nhead=2,  # the number of heads in the multiheadattention models\n",
    "    dim_feedforward=32,  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    num_layers=3,\n",
    "    dropout=0.1,\n",
    "    classifier_dropout=0.1,\n",
    ")\n",
    "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b72f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrankfundel\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ffundel/BAT/models/wandb/run-20220415_152829-5ntogk00</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/frankfundel/BAT-1/runs/5ntogk00\" target=\"_blank\">cosmic-violet-9</a></strong> to <a href=\"https://wandb.ai/frankfundel/BAT-1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Starting at epoch 0 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/562 [00:00<?, ?it/s]/home/ffundel/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in MmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/opt/Python/3.8.6/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/Python/3.8.6/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 668, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/Python/3.8.6/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/Python/3.8.6/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/Python/3.8.6/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 456, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 445, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 352, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 647, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 335, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1542671/2060154334.py\", line 24, in <cell line: 20>\n",
      "    train_loss, train_acc = train_epoch(model, epoch, criterion, optimizer, train_loader, device)\n",
      "  File \"/tmp/ipykernel_1542671/2623445104.py\", line 19, in train_epoch\n",
      "    outputs = model(inputs)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_1542671/1144719350.py\", line 47, in forward\n",
      "    x = self.transformer_encoder(x)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 202, in forward\n",
      "    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 344, in forward\n",
      "    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\", line 352, in _sa_block\n",
      "    x = self.self_attn(x, x, x,\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py\", line 1038, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 5245, in multi_head_attention_forward\n",
      "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
      "  File \"/home/ffundel/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 4928, in _in_projection_packed\n",
      "    return linear(q, w, b).chunk(3, dim=-1)\n",
      " (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:104.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  0%|                                                   | 0/562 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 7.93 GiB total capacity; 6.68 GiB already allocated; 592.44 MiB free; 6.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==================== Starting at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ====================\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m Acc: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_loss, train_acc), flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m test_epoch(model, epoch, criterion, optimizer, val_loader, device)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, epoch, criterion, optimizer, dataloader, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate gradients\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Update Weights\u001b[39;00m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 7.93 GiB total capacity; 6.68 GiB already allocated; 592.44 MiB free; 6.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"BAT-1\", entity=\"frankfundel\")\n",
    "\n",
    "wandb.config = {\n",
    "  \"learning_rate\": lr,\n",
    "  \"epochs\": epochs,\n",
    "  \"batch_size\": batch_size\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), lr=lr\n",
    ")\n",
    "\n",
    "min_val_loss = np.inf\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(epochs):\n",
    "    end = time.time()\n",
    "    print(f\"==================== Starting at epoch {epoch} ====================\", flush=True)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, epoch, criterion, optimizer, train_loader, device)\n",
    "    print('Training loss: {:.4f} Acc: {:.4f}'.format(train_loss, train_acc), flush=True)\n",
    "    \n",
    "    val_loss, val_acc = test_epoch(model, epoch, criterion, optimizer, val_loader, device)\n",
    "    print('Validation loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_acc), flush=True)\n",
    "    \n",
    "    if use_reduceonplateu:\n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "    })\n",
    "    \n",
    "    if min_val_loss > val_loss:\n",
    "        print('val_loss decreased, saving model', flush=True)\n",
    "        min_val_loss = val_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'bat-1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b28828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
