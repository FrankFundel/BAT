{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105682a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../datasets/')\n",
    "from prepare_sequences import prepare, germanBats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = germanBats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b788dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:37<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "num_bands = 257\n",
    "patch_len = 44                               # = 250ms ~ 25ms\n",
    "patch_skip = patch_len / 2                   # = 150ms ~ 15ms\n",
    "\n",
    "resize = None\n",
    "\n",
    "mode = 'slide'\n",
    "options = {\n",
    "    'seq_len': 60,                            # = 500ms with ~ 5 calls\n",
    "    'seq_skip': 15,\n",
    "}\n",
    "\n",
    "X_test, Y_test = prepare(\"../../datasets/prepared.h5\", classes, patch_len, patch_skip,\n",
    "                                                         options, mode, resize, only_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a08bfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 4979\n",
      "(4979, 60, 44, 257) (4979,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total sequences:\", len(X_test))\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d07e27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb1806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, num_classes, on_value=1., off_value=0., device='cuda'):\n",
    "    x = x.long().view(-1, 1)\n",
    "    return torch.full((x.size()[0], num_classes), off_value, device=device).scatter_(1, x, on_value)\n",
    "\n",
    "def mixup(x, y, num_classes):\n",
    "    x_flipped = x.flip(0)\n",
    "    x.add_(x_flipped)\n",
    "    y1 = one_hot(y, num_classes, device=x.device)\n",
    "    y2 = one_hot(y.flip(0), num_classes, device=x.device)\n",
    "    return x, y1 + y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447933a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_classes = len(list(classes))\n",
    "\n",
    "test_len = len(X_test) - len(X_test) % 2\n",
    "test_data = TensorDataset(torch.Tensor(X_test[:test_len]), torch.from_numpy(Y_test[:test_len]))\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ed28fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def stitch(a, r):\\n    return a[::r]\\n  \\ndef plot_sequence(seq, y):\\n    plt.figure(figsize = (20, 2.5))\\n    stitched = stitch(seq, int(patch_len / patch_skip))\\n    spec = np.rot90(np.concatenate(stitched))\\n    plt.imshow(spec, interpolation=\\'nearest\\', aspect=\\'auto\\', cmap=\\'inferno\\')\\n    plt.colorbar()\\n    label_list = []\\n    if(len(y.shape) > 0):\\n        for i in np.argsort(-y)[:2]:\\n            label_list.append(list(classes)[i])\\n        plt.title(\", \".join(label_list))\\n    else:\\n        plt.title(list(classes)[y])\\n\\nk = 3\\nX1, Y1 = next(iter(test_loader))\\nprint(X1.shape, Y1.shape)\\nplot_sequence(X1[k].detach().numpy(), Y1[k].detach().numpy())\\nplot_sequence(X1[-k-1].detach().numpy(), Y1[-k-1].detach().numpy())\\n\\nX1, Y1 = mixup(X1, Y1, num_classes=num_classes)\\nplot_sequence(X1[k].detach().numpy(), Y1[k].detach().numpy())'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def stitch(a, r):\n",
    "    return a[::r]\n",
    "  \n",
    "def plot_sequence(seq, y):\n",
    "    plt.figure(figsize = (20, 2.5))\n",
    "    stitched = stitch(seq, int(patch_len / patch_skip))\n",
    "    spec = np.rot90(np.concatenate(stitched))\n",
    "    plt.imshow(spec, interpolation='nearest', aspect='auto', cmap='inferno')\n",
    "    plt.colorbar()\n",
    "    label_list = []\n",
    "    if(len(y.shape) > 0):\n",
    "        for i in np.argsort(-y)[:2]:\n",
    "            label_list.append(list(classes)[i])\n",
    "        plt.title(\", \".join(label_list))\n",
    "    else:\n",
    "        plt.title(list(classes)[y])\n",
    "\n",
    "k = 3\n",
    "X1, Y1 = next(iter(test_loader))\n",
    "print(X1.shape, Y1.shape)\n",
    "plot_sequence(X1[k].detach().numpy(), Y1[k].detach().numpy())\n",
    "plot_sequence(X1[-k-1].detach().numpy(), Y1[-k-1].detach().numpy())\n",
    "\n",
    "X1, Y1 = mixup(X1, Y1, num_classes=num_classes)\n",
    "plot_sequence(X1[k].detach().numpy(), Y1[k].detach().numpy())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41131d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from bat_2 import Net\n",
    "\n",
    "max_len = 60\n",
    "d_model = 64 \n",
    "\n",
    "nhead = 2\n",
    "dim_feedforward = 32\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "classifier_dropout = 0.3\n",
    "\n",
    "model = Net(\n",
    "    max_len=max_len,\n",
    "    patch_dim=resize[0]*resize[1] if resize is not None else patch_len * num_bands, # patch_len * num_bands, # 44 * 257 = 11,308\n",
    "    d_model=d_model,\n",
    "    num_classes=len(list(classes)),\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    classifier_dropout=classifier_dropout,\n",
    ")\n",
    "model.load_state_dict(torch.load('bat_2_convnet.pth'))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "    \n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de5ed824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=ResNet\n",
       "  (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "  (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "  (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "  (maxpool): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "  (layer1): RecursiveScriptModule(\n",
       "    original_name=Sequential\n",
       "    (0): RecursiveScriptModule(\n",
       "      original_name=Block\n",
       "      (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv3): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn3): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "      (identity_downsample): RecursiveScriptModule(\n",
       "        original_name=Sequential\n",
       "        (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "        (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "    (1): RecursiveScriptModule(\n",
       "      original_name=Block\n",
       "      (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv3): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn3): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "    )\n",
       "  )\n",
       "  (layer2): RecursiveScriptModule(\n",
       "    original_name=Sequential\n",
       "    (0): RecursiveScriptModule(\n",
       "      original_name=Block\n",
       "      (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv3): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn3): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "      (identity_downsample): RecursiveScriptModule(\n",
       "        original_name=Sequential\n",
       "        (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "        (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "    (1): RecursiveScriptModule(\n",
       "      original_name=Block\n",
       "      (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv3): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn3): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "    )\n",
       "  )\n",
       "  (layer3): RecursiveScriptModule(\n",
       "    original_name=Sequential\n",
       "    (0): RecursiveScriptModule(\n",
       "      original_name=Block\n",
       "      (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv3): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn3): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "      (identity_downsample): RecursiveScriptModule(\n",
       "        original_name=Sequential\n",
       "        (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "        (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "    (1): RecursiveScriptModule(\n",
       "      original_name=Block\n",
       "      (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv3): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn3): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "    )\n",
       "  )\n",
       "  (layer4): RecursiveScriptModule(\n",
       "    original_name=Sequential\n",
       "    (0): RecursiveScriptModule(\n",
       "      original_name=Block\n",
       "      (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv3): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn3): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "      (identity_downsample): RecursiveScriptModule(\n",
       "        original_name=Sequential\n",
       "        (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "        (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "    (1): RecursiveScriptModule(\n",
       "      original_name=Block\n",
       "      (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn2): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (conv3): RecursiveScriptModule(original_name=Conv2d)\n",
       "      (bn3): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "      (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): RecursiveScriptModule(original_name=AdaptiveAvgPool2d)\n",
       "  (fc): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_nocall_model = torch.jit.load('../call_nocall/call_nocall.pt')\n",
    "call_nocall_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86577d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4978/4978 [01:48<00:00, 45.74it/s]\n",
      "  0%|▎                                                                                                                                                     | 10/4978 [00:00<00:50, 97.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.8049417436721575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 3264/4978 [00:38<00:20, 85.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m mixup(inputs, labels, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m---> 30\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Feed Network\u001b[39;00m\n\u001b[1;32m     31\u001b[0m prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(output, \u001b[38;5;241m1\u001b[39m)[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     32\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(labels, \u001b[38;5;241m1\u001b[39m)[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/BAT/models/mixed_tests/bat_2.py:144\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder\n\u001b[1;32m    143\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m--> 144\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    147\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/BAT/models/mixed_tests/bat_2.py:72\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attn, ff \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 72\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     73\u001b[0m         x \u001b[38;5;241m=\u001b[39m ff(x) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/BAT/models/mixed_tests/bat_2.py:14\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/BAT/models/mixed_tests/bat_2.py:59\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn, v)\n\u001b[1;32m     58\u001b[0m out \u001b[38;5;241m=\u001b[39m rearrange(out, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb h n d -> b n (h d)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "corrects = 0.0\n",
    "mixed_corrects = 0.0\n",
    "\n",
    "model.eval()\n",
    "call_nocall_model.eval()\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in tqdm.tqdm(test_loader):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    cnc_outputs = call_nocall_model(inputs[0].unsqueeze(1))\n",
    "    cnc_pred = torch.argmax(cnc_outputs, 1) # call indices\n",
    "    \n",
    "    if cnc_pred.nonzero().shape[0] > 1:\n",
    "        output = model(inputs) # Feed Network\n",
    "        prediction = torch.argmax(output, 1)\n",
    "        corrects += (prediction == labels).sum().item()\n",
    "    \n",
    "print(\"Test acc:\", corrects / (len(test_data)))\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in tqdm.tqdm(test_loader):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    inputs, labels = mixup(inputs, labels, num_classes=num_classes)\n",
    "    \n",
    "    output = model(inputs.to(device)) # Feed Network\n",
    "    prediction = torch.argsort(output, 1)[:,-2:]\n",
    "    target = torch.argsort(labels, 1)[:,-2:]\n",
    "    mixed_corrects += (torch.count_nonzero(prediction == target, dim=1) + \\\n",
    "                       torch.count_nonzero(prediction.flip(1) == target, dim=1)).sum().item()\n",
    "    \n",
    "print(\"Mixed test acc:\", mixed_corrects / (len(test_data) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1c633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
